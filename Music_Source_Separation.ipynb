{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music Source Separation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_KHdBaFRHP2EpICwazP74WAQ2w4zowj8",
      "authorship_tag": "ABX9TyNuMsIKx8n+0pUAgjQXPlmH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrpep/music-source-separation-4all/blob/main/Music_Source_Separation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx7C_lx-AUBh"
      },
      "source": [
        "!pip install torchaudio==0.10.0+cu111 torch==1.10.0+cu111 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n",
        "!pip install demucs\n",
        "!pip install youtube-dl\n",
        "!pip install ffmpeg-python\n",
        "!pip install openunmix\n",
        "!pip install typer\n",
        "!pip install httpx[http2]==0.19.0\n",
        "!pip install --no-deps spleeter\n",
        "!git clone https://github.com/pfnet-research/meta-tasnet\n",
        "!wget \"https://www.dropbox.com/s/zw6zgt3edd88v87/best_model.pt\"\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJlHalWL--PC",
        "cellView": "form"
      },
      "source": [
        "#@title Enter nerd mode\n",
        "from demucs.pretrained import get_model\n",
        "from demucs.apply import apply_model\n",
        "import torch\n",
        "from youtube_dl import YoutubeDL\n",
        "import ffmpeg\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "import sys\n",
        "sys.path.append(\"/content/meta-tasnet\")\n",
        "\n",
        "from model.tasnet import MultiTasNet\n",
        "import librosa\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "def download_from_youtube(url,start,end,sr=44100):\n",
        "  with YoutubeDL(dict(format='bestaudio')) as ydl:\n",
        "    video_url = ydl.extract_info(url, download=False)['formats'][0]['url']\n",
        "  kwargs = {}\n",
        "  if start > 0:\n",
        "    kwargs['ss'] = start\n",
        "  if end > 0:\n",
        "    kwargs['t'] = end - start\n",
        "  out,_ = ffmpeg.input(video_url,**kwargs).output('-', format='s16le', acodec='pcm_s16le', ac=2, ar=sr).overwrite_output().run(capture_stdout=True)\n",
        "  y = np.frombuffer(out,dtype='int16')\n",
        "  y = np.reshape(y,(len(y)//2,2))/(2**15 - 1)\n",
        "  return y\n",
        "\n",
        "def demucs_separate(x, shifts=1, models=None):\n",
        "  if 'mdx_extra_q' in models:\n",
        "    model = models['mdx_extra_q']\n",
        "  else:\n",
        "    model = get_model(name='mdx_extra_q')\n",
        "    models['mdx_extra_q'] = model\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  x = x.T\n",
        "  x = np.expand_dims(x,0)\n",
        "  x = torch.from_numpy(x)\n",
        "  x = x.to(device, dtype=torch.float32)\n",
        "  ref = x.mean(0)\n",
        "  x = (x - ref.mean())/ref.std()\n",
        "  sources = apply_model(model,x,shifts=shifts,split=True,overlap=0.25,progress=True)[0]\n",
        "  sources = sources * ref.std() + ref.mean()\n",
        "  sources = sources.detach().to('cpu').numpy()\n",
        "\n",
        "  return sources\n",
        "\n",
        "def openumx_separate(x,split_size=30, split_overlap=1, models=None):\n",
        "  if 'umxl' in models:\n",
        "    separator = models['umxl']\n",
        "  else:\n",
        "    separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxl', device='cuda')\n",
        "    models['umxl'] = separator\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  separator.to(device)\n",
        "  fs = 44100\n",
        "  x = x.T\n",
        "  x = x[np.newaxis,:,:]\n",
        "  split_frames = int(split_size*fs)\n",
        "  x_splits = np.concatenate([x[:,:,i:i+split_frames] for i in range(0,x.shape[-1]-split_frames,int(split_frames*split_overlap))],axis=0)\n",
        "  sources_splits = []\n",
        "  for split in x_splits:\n",
        "    sources = separator(torch.tensor(split).unsqueeze(0).to(device,dtype=torch.float32))\n",
        "    sources = sources.detach().cpu().numpy()\n",
        "    sources_splits.append(sources)\n",
        "  sources = np.concatenate(sources_splits,axis = -1)\n",
        "  return sources\n",
        "\n",
        "def metatasnet_separate(x,models=None):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  rate=44100\n",
        "  if 'metatasnet' in models:\n",
        "    model = models['metatasnet']\n",
        "  else:\n",
        "    state = torch.load(\"best_model.pt\")  # load checkpoint\n",
        "    model = MultiTasNet(state[\"args\"]).to(device)  # initialize the model\n",
        "    model.load_state_dict(state['state_dict'])  # load weights from the checkpoint\n",
        "  \n",
        "  def resample(audio, target_rate):\n",
        "    return librosa.core.resample(audio, rate, target_rate, res_type='kaiser_best', fix=False)\n",
        "    \n",
        "  \n",
        "  x = x.astype('float32')  # match the type with the type of the weights in the network\n",
        "  x = x.T\n",
        "  mix = [resample(x, s) for s in[8000, 16000, 32000]]  # resample to different sampling rates for the three stages\n",
        "  mix = [librosa.util.fix_length(m, (mix[0].shape[-1]+1)*(2**i)) for i,m in enumerate(mix)]  # allign all three sample so that their lenghts are divisible\n",
        "  mix = [torch.from_numpy(s).float().to(device).view(1, 1, -1) for s in mix]  # cast to tensor with shape: [1, 1, T']\n",
        "  mix = [s / s.std(dim=-1, keepdim=True) for s in mix]  # normalize by the standard deviation\n",
        "\n",
        "  model.eval()\n",
        "  n_chunks = x.shape[0]//(30*44100)\n",
        "  with torch.no_grad():        \n",
        "    sources = model.inference(mix, n_chunks=n_chunks)[-1]  # call the network to obtain the separated audio with shape [1, 4, 1, T']\n",
        "\n",
        "  # normalize the amplitudes by computing the least squares\n",
        "  # -> we try to scale the separated stems so that their sum is equal to the input mix \n",
        "  a = sources[0,:,0,:].cpu().numpy().T  # separated stems\n",
        "  b = mix[-1][0,0,:].cpu().numpy()  # input mix\n",
        "  sol = np.linalg.lstsq(a, b, rcond=None)[0]  # scaling coefficients that minimize the MSE\n",
        "  sources = a * sol  # scale the separated stems\n",
        "\n",
        "  return sources\n",
        "\n",
        "def spleeter_separate(x):\n",
        "  pass\n",
        "\n",
        "yt_cache_path = Path('youtube_cache')\n",
        "if not yt_cache_path.exists():\n",
        "  yt_cache_path.mkdir(parents=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpdTgpAZAcbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "a91aab8d-5eba-4b81-d563-90dea8f460da"
      },
      "source": [
        "#@title Separate!\n",
        "model = \"demucs\" #@param [\"demucs\", \"open-umx\",\"meta-tasnet\",\"spleeter\"]\n",
        "youtube_link = \"GNXsl13WT_U\" #@param {type:\"string\"}\n",
        "youtube_start = 255 #@param {type:\"integer\"}\n",
        "youtube_end = 315 #@param {type:\"integer\"}\n",
        "\n",
        "sampling_rate=44100\n",
        "models = {}\n",
        "\n",
        "if youtube_link == \"\":\n",
        "  from google.colab import files \n",
        "  uploaded = files.upload()\n",
        "  mix = librosa.core.load(uploaded,sr=sampling_rate)\n",
        "else:\n",
        "  video_cache_path = Path(yt_cache_path,'{}.wav'.format(youtube_link))\n",
        "  if video_cache_path.exists():\n",
        "    print('Cacheando video...')\n",
        "    mix, _ = sf.read(str(video_cache_path.absolute()))\n",
        "  else:\n",
        "    print('Bajando de youtube...')\n",
        "    mix = download_from_youtube(youtube_link, youtube_start,youtube_end,sr=sampling_rate)\n",
        "    sf.write(video_cache_path,mix,samplerate=sampling_rate)\n",
        "if model == 'demucs':\n",
        "  sources = demucs_separate(mix,models=models)\n",
        "  source_names = ['Drums','Bass','Other','Vocals']\n",
        "  source_fs=44100\n",
        "elif model == 'open-umx':\n",
        "  sources = openumx_separate(mix,models=models)[0]\n",
        "  source_names = ['Vocals','Drums','Bass','Other']\n",
        "  source_fs=44100\n",
        "elif model == 'meta-tasnet':\n",
        "  if mix.ndim == 2:\n",
        "    ch_sources = []\n",
        "    for ch in mix.T:\n",
        "      ch_i = metatasnet_separate(ch,models)\n",
        "      ch_sources.append(np.expand_dims(ch_i,0))\n",
        "    sources = np.concatenate(ch_sources,axis=0)\n",
        "    sources = np.transpose(sources,(2,0,1))\n",
        "  else:\n",
        "    sources = metatasnet_separate(ch,models)\n",
        "  source_names = ['Bass','Drums','Vocals','Other']\n",
        "  source_fs=32000\n",
        "\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "if not Path('outputs/{}/{}'.format(model,youtube_link)).exists():\n",
        "  Path('outputs/{}/{}'.format(model,youtube_link)).mkdir(parents=True)\n",
        "for source_name, source in zip(source_names,sources):\n",
        "  sf.write('outputs/{}/{}/{}.wav'.format(model,youtube_link,source_name),source.T,source_fs)\n",
        "  #print(source_name)\n",
        "  #display(Audio(source,rate=44100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bajando de youtube...\n",
            "[youtube] GNXsl13WT_U: Downloading webpage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████| 66.0/66.0 [00:03<00:00, 19.73seconds/s]\n",
            "100%|██████████████████████████████████████████████████████████████████████████| 66.0/66.0 [00:03<00:00, 19.31seconds/s]\n",
            "100%|██████████████████████████████████████████████████████████████████████████| 66.0/66.0 [00:03<00:00, 19.52seconds/s]\n",
            "100%|██████████████████████████████████████████████████████████████████████████| 66.0/66.0 [00:03<00:00, 20.08seconds/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKNW0N6-Ecd-"
      },
      "source": [
        "### Transcripcion multipista"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG7uRntZK8LL",
        "outputId": "281aa2a5-aadb-4e07-a9e6-dec9f5909449"
      },
      "source": [
        "!pip install --no-deps omnizart\n",
        "!pip install pretty_midi\n",
        "!omnizart download-checkpoints\n",
        "!pip install mido\n",
        "!pip install madmom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting omnizart\n",
            "  Downloading omnizart-0.4.2-py3-none-any.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 3.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: omnizart\n",
            "Successfully installed omnizart-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKD3PmxhdW1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053c24d1-f497-4485-9cee-b3a6b5121350"
      },
      "source": [
        "from omnizart.music import app as mapp\n",
        "from omnizart.drum import app as dapp\n",
        "mapp.transcribe('outputs/demucs/GNXsl13WT_U/Bass.wav')\n",
        "dapp.transcribe('outputs/demucs/GNXsl13WT_U/Drums.wav')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-25 21:14:46 Loading model...\n",
            "2021-11-25 21:14:46 Using built-in model /usr/local/lib/python3.7/dist-packages/omnizart/checkpoints/music/music_piano for transcription.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-25 21:15:00 Extracting feature...\n",
            "2021-11-25 21:15:20 Predicting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-25 21:15:23 Infering notes....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-25 21:15:24 MIDI file has been written to ./Bass.mid.\n",
            "2021-11-25 21:15:24 Transcription finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pitch: 1/88\rPitch: 2/88\rPitch: 3/88\rPitch: 4/88\rPitch: 5/88\rPitch: 6/88\rPitch: 7/88\rPitch: 8/88\rPitch: 9/88\rPitch: 10/88\rPitch: 11/88\rPitch: 12/88\rPitch: 13/88\rPitch: 14/88\rPitch: 15/88\rPitch: 16/88\rPitch: 17/88\rPitch: 18/88\rPitch: 19/88\rPitch: 20/88\rPitch: 21/88\rPitch: 22/88\rPitch: 23/88\rPitch: 24/88\rPitch: 25/88\rPitch: 26/88\rPitch: 27/88\rPitch: 28/88\rPitch: 29/88\rPitch: 30/88\rPitch: 31/88\rPitch: 32/88\rPitch: 33/88\rPitch: 34/88\rPitch: 35/88\rPitch: 36/88\rPitch: 37/88\rPitch: 38/88\rPitch: 39/88\rPitch: 40/88\rPitch: 41/88\rPitch: 42/88\rPitch: 43/88\rPitch: 44/88\rPitch: 45/88\rPitch: 46/88\rPitch: 47/88\rPitch: 48/88\rPitch: 49/88\rPitch: 50/88\rPitch: 51/88\rPitch: 52/88\rPitch: 53/88\rPitch: 54/88\rPitch: 55/88\rPitch: 56/88\rPitch: 57/88\rPitch: 58/88\rPitch: 59/88\rPitch: 60/88\rPitch: 61/88\rPitch: 62/88\rPitch: 63/88\rPitch: 64/88\rPitch: 65/88\rPitch: 66/88\rPitch: 67/88\rPitch: 68/88\rPitch: 69/88\rPitch: 70/88\rPitch: 71/88\rPitch: 72/88\rPitch: 73/88\rPitch: 74/88\rPitch: 75/88\rPitch: 76/88\rPitch: 77/88\rPitch: 78/88\rPitch: 79/88\rPitch: 80/88\rPitch: 81/88\rPitch: 82/88\rPitch: 83/88\rPitch: 84/88\rPitch: 85/88\rPitch: 86/88\rPitch: 87/88\rPitch: 88/88\r                                                                                \r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmXHZUojHbxu"
      },
      "source": [
        "#Datas\n",
        "\n",
        "\n",
        "\n",
        "Music demixing challenge: https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021/leaderboards\n",
        "\n",
        "Demucs: https://github.com/facebookresearch/demucs\n",
        "OpenUnmix: https://sigsep.github.io/open-unmix/\n",
        "Meta-Tasnet: https://github.com/pfnet-research/meta-tasnet\n"
      ]
    }
  ]
}